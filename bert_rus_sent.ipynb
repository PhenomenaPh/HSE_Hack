{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "cellId": "vfw6c2ih6fvtgcw0zax7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-04-09 10:10:32--  https://raw.githubusercontent.com/shitkov/bert4classification/main/bert_classifier.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5278 (5.2K) [text/plain]\n",
      "Saving to: ‘bert_classifier.py’\n",
      "\n",
      "     0K .....                                                 100% 31.5M=0s\n",
      "\n",
      "2023-04-09 10:10:32 (31.5 MB/s) - ‘bert_classifier.py’ saved [5278/5278]\n",
      "\n",
      "--2023-04-09 10:10:32--  https://raw.githubusercontent.com/shitkov/bert4classification/main/bert_dataset.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 891 [text/plain]\n",
      "Saving to: ‘bert_dataset.py’\n",
      "\n",
      "     0K                                                       100% 13.0M=0s\n",
      "\n",
      "2023-04-09 10:10:32 (13.0 MB/s) - ‘bert_dataset.py’ saved [891/891]\n",
      "\n",
      "--2023-04-09 10:10:32--  https://raw.githubusercontent.com/shitkov/bert4classification/main/requirements.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 103 [text/plain]\n",
      "Saving to: ‘requirements.txt.1’\n",
      "\n",
      "     0K                                                       100% 4.31M=0s\n",
      "\n",
      "2023-04-09 10:10:33 (4.31 MB/s) - ‘requirements.txt.1’ saved [103/103]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "!wget https://raw.githubusercontent.com/shitkov/bert4classification/main/bert_classifier.py\n",
    "!wget https://raw.githubusercontent.com/shitkov/bert4classification/main/bert_dataset.py\n",
    "!wget https://raw.githubusercontent.com/shitkov/bert4classification/main/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellId": "td7k32039v4ojxhe98afu",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch==1.8.1\n",
      "  Downloading torch-1.8.1-cp38-cp38-manylinux1_x86_64.whl (804.1 MB)\n",
      "     |████████████████████████████████| 804.1 MB 1.9 kB/s              \n",
      "\u001b[?25hCollecting sentencepiece==0.1.95\n",
      "  Downloading sentencepiece-0.1.95-cp38-cp38-manylinux2014_x86_64.whl (1.2 MB)\n",
      "     |████████████████████████████████| 1.2 MB 60.0 MB/s            \n",
      "\u001b[?25hCollecting transformers==4.5.1\n",
      "  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\n",
      "     |████████████████████████████████| 2.1 MB 107.2 MB/s            \n",
      "\u001b[?25hCollecting pandas==1.1.5\n",
      "  Downloading pandas-1.1.5-cp38-cp38-manylinux1_x86_64.whl (9.3 MB)\n",
      "     |████████████████████████████████| 9.3 MB 76.3 MB/s            \n",
      "\u001b[?25hCollecting numpy==1.19.5\n",
      "  Downloading numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
      "     |████████████████████████████████| 14.9 MB 118.2 MB/s            \n",
      "\u001b[?25hCollecting scikit-learn==0.22.2\n",
      "  Downloading scikit_learn-0.22.2-cp38-cp38-manylinux1_x86_64.whl (7.0 MB)\n",
      "     |████████████████████████████████| 7.0 MB 75.8 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.8.1->-r requirements.txt (line 1)) (3.7.4.3)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.1->-r requirements.txt (line 3)) (0.10.3)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.1->-r requirements.txt (line 3)) (0.0.46)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.1->-r requirements.txt (line 3)) (4.50.0)\n",
      "Requirement already satisfied: packaging in /kernel/lib/python3.8/site-packages (from transformers==4.5.1->-r requirements.txt (line 3)) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.1->-r requirements.txt (line 3)) (2021.11.10)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.1->-r requirements.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: requests in /kernel/lib/python3.8/site-packages (from transformers==4.5.1->-r requirements.txt (line 3)) (2.28.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.8/dist-packages (from pandas==1.1.5->-r requirements.txt (line 4)) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /kernel/lib/python3.8/site-packages (from pandas==1.1.5->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==0.22.2->-r requirements.txt (line 6)) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==0.22.2->-r requirements.txt (line 6)) (1.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /kernel/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas==1.1.5->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /kernel/lib/python3.8/site-packages (from packaging->transformers==4.5.1->-r requirements.txt (line 3)) (2.4.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /kernel/lib/python3.8/site-packages (from requests->transformers==4.5.1->-r requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /kernel/lib/python3.8/site-packages (from requests->transformers==4.5.1->-r requirements.txt (line 3)) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /kernel/lib/python3.8/site-packages (from requests->transformers==4.5.1->-r requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.8/site-packages (from requests->transformers==4.5.1->-r requirements.txt (line 3)) (2022.12.7)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.5.1->-r requirements.txt (line 3)) (8.0.3)\n",
      "Installing collected packages: numpy, transformers, torch, sentencepiece, scikit-learn, pandas\n",
      "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.8 are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script transformers-cli is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx and convert-onnx-to-caffe2 are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.10.1+cu111 requires torch==1.9.1, but you have torch 1.8.1 which is incompatible.\n",
      "torchaudio 0.9.1 requires torch==1.9.1, but you have torch 1.8.1 which is incompatible.\n",
      "tensorflow 2.6.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
      "tensorflow 2.6.0 requires wrapt~=1.12.1, but you have wrapt 1.15.0 which is incompatible.\u001b[0m\n",
      "Successfully installed numpy-1.19.5 pandas-1.1.5 scikit-learn-0.22.2 sentencepiece-0.1.95 torch-1.8.1 transformers-4.5.1\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "cellId": "2xxpq9qb3w6hk6phjr7jk9"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "cellId": "uf3p6mhaa33e9r4s5ga59"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "TEST_SIZE = 0.2 # Test size for metric check\n",
    "TIMEOUT = 9000 # Time in seconds for automl run\n",
    "TARGET_NAME = 'sentiment' # Target column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "cellId": "8dirtmxzfnukha765z78pc"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train = pd.read_csv('/home/jupyter/mnt/s3/mikezz21datahackathon/train.csv')\n",
    "\n",
    "train_sent = train.loc[:,['sentence', 'sentiment']]\n",
    "train_sent = train_sent[train_sent.duplicated()==False]\n",
    "duplicates = train_sent[train_sent.duplicated('sentence', keep = False)==True].sort_values(by = 'sentence')\n",
    "train_sent = train_sent[train_sent.duplicated('sentence', keep = False)==False]\n",
    "\n",
    "train_sent['sentiment'] = train_sent['sentiment'].replace({'+': 1, '−': 2, '?': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "cellId": "51vqfbbelujzn97p8ti78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splitted. Parts sizes: train_data = (4791, 2), test_data = (1198, 2)\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_data, test_data = train_test_split(train_sent, \n",
    "                                         test_size=TEST_SIZE, \n",
    "                                         stratify=train_sent[TARGET_NAME], \n",
    "                                         random_state=10)\n",
    "\n",
    "print('Data splitted. Parts sizes: train_data = {}, test_data = {}'\n",
    "              .format(train_data.shape, test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "cellId": "71h58rjfr09t2g015m0iz8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splitted. Parts sizes: train = (3832, 2), valid = (959, 2)\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "train, valid = train_test_split(train_data, \n",
    "                                         test_size=TEST_SIZE, \n",
    "                                         stratify=train_data[TARGET_NAME], \n",
    "                                         random_state=10)\n",
    "\n",
    "print('Data splitted. Parts sizes: train = {}, valid = {}'\n",
    "              .format(train.shape, valid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "cellId": "791lpd1rexcb8f2xo0f79q"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from bert_dataset import CustomDataset\n",
    "from bert_classifier import BertClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "cellId": "94qm7lhkvyhbsjfevz27nv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/LaBSE-en-ru were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/LaBSE-en-ru and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "classifier = BertClassifier(\n",
    "        model_path='cointegrated/LaBSE-en-ru',\n",
    "        tokenizer_path='cointegrated/LaBSE-en-ru',\n",
    "        n_classes=3,\n",
    "        epochs=5,\n",
    "        model_save_path='bert.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "cellId": "r8iql6onizoq70zl79i8q"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "classifier.preparation(\n",
    "        X_train=list(train['sentence']),\n",
    "        y_train=list(train['sentiment']),\n",
    "        X_valid=list(valid['sentence']),\n",
    "        y_valid=list(valid['sentiment'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "cellId": "ue2ux1767rqmf3x401q7p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train loss 0.54733802481197 accuracy 0.8776096033402921\n",
      "Val loss 0.43075253869659114 accuracy 0.9113660062565172\n",
      "----------\n",
      "Epoch 2/5\n",
      "Train loss 0.26783889846434145 accuracy 0.949634655532359\n",
      "Val loss 0.32785549921754864 accuracy 0.9311783107403545\n",
      "----------\n",
      "Epoch 3/5\n",
      "Train loss 0.14831838606348388 accuracy 0.972599164926931\n",
      "Val loss 0.42455433977138457 accuracy 0.9249217935349322\n",
      "----------\n",
      "Epoch 4/5\n",
      "Train loss 0.08159848449346226 accuracy 0.98643006263048\n",
      "Val loss 0.40367167137431653 accuracy 0.9374348279457769\n",
      "----------\n",
      "Epoch 5/5\n",
      "Train loss 0.034665543237426696 accuracy 0.9942588726513569\n",
      "Val loss 0.4086671526201144 accuracy 0.9374348279457769\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "cellId": "vsodqcp4r9ir0uekfep00h"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "data_participants = pd.read_csv('1sentencenewtest_sentence.csv')\n",
    "texts = list(data_participants['sentence'])\n",
    "predictions = [classifier.predict(t) for t in texts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "cellId": "y3f2en85piy5nhds58wn9"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import torch.nn as nn\n",
    "x = nn.functional.softmax(torch.cat(predictions,dim=0), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "cellId": "d4zy72135ovit2urry6cmj"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "list_of_prob = x.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "cellId": "ctdrlda9dht50fabopy6q"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999994"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "list_of_prob[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "cellId": "fqqj6rhk7pl8j4gk7c6gdm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9700244309584657"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(labels, list_of_prob, multi_class = 'ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "cellId": "3sf650vqkmq2jypvluwbtu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.2715397e-05, 2.3473191e-05, 9.9992383e-01],\n",
       "       [5.3600375e-05, 9.9987447e-01, 7.1932722e-05],\n",
       "       [4.9991959e-05, 2.3441646e-05, 9.9992657e-01],\n",
       "       ...,\n",
       "       [4.0504269e-04, 9.9950147e-01, 9.3482835e-05],\n",
       "       [9.5448602e-05, 3.7025133e-05, 9.9986756e-01],\n",
       "       [5.0740291e-05, 9.9988163e-01, 6.7582754e-05]], dtype=float32)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "list_of_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "cellId": "yx3wmb6nfogm4csnzg8xw"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "data_participants['prob_?'] = list_of_prob[:,0]\n",
    "data_participants['prob_+'] = list_of_prob[:,1]\n",
    "data_participants['prob_-'] = list_of_prob[:,2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "cellId": "82ygq0guvw9rtjexpzkqyk"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "data_participants.to_csv('data_participants_with_probs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "cellId": "xsqs0egy9z2t1tanrn3"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train.to_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "cellId": "bmczksns2s5dxrr0roumca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.8553061796560385, recall: 0.8485804896132582, f1score: 0.8516182921411679\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1score = precision_recall_fscore_support(labels, predictions,average='macro')[:3]\n",
    "\n",
    "print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "0svbfiy6auggertlvyguk3a"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "f49ad688-fd47-4943-a96f-814fc95a11a3",
  "notebookPath": "welcome_ru.ipynb",
  "ydsNotebookPath": "welcome_ru.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
