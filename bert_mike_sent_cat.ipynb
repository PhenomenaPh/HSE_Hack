{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "7ly6bw5d5aosz4q54c5ax",
    "id": "U57Hx7J0CgFq"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "vfm70t08zed84w6f8sca1h"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cellId": "ik9oc132mgv746yv29o3h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.14.0)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.14.2-py3-none-any.whl (2.0 MB)\n",
      "     |████████████████████████████████| 2.0 MB 1.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.50.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: packaging>=20.0 in /kernel/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: requests in /kernel/lib/python3.8/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: setuptools in /kernel/lib/python3.8/site-packages (from wandb) (51.0.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.1)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.4.4)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /kernel/lib/python3.8/site-packages (from wandb) (5.7.3)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.1.24)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from wandb) (3.7.4.3)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.19.1-py2.py3-none-any.whl (199 kB)\n",
      "     |████████████████████████████████| 199 kB 117.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: six>=1.4.0 in /kernel/lib/python3.8/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /kernel/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /kernel/lib/python3.8/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /kernel/lib/python3.8/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /kernel/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "Building wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8785 sha256=8e42634d87465da2981e42e076b9179b47427216d9f6f4072448a10f32995150\n",
      "  Stored in directory: /tmp/xdg_cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
      "Successfully built pathtools\n",
      "Installing collected packages: setproctitle, sentry-sdk, pathtools, docker-pycreds, wandb\n",
      "\u001b[33m  WARNING: The scripts wandb and wb are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed docker-pycreds-0.4.0 pathtools-0.1.2 sentry-sdk-1.19.1 setproctitle-1.3.2 wandb-0.14.2\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "cellId": "f0i85knru2xrju5m6p888",
    "id": "ohoSNHCwCkf-"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import re\n",
    "from warnings import filterwarnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset \n",
    "from transformers import AutoModel, AdamW, AutoTokenizer, get_linear_schedule_with_warmup,AutoConfig\n",
    "import wandb\n",
    "\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "cellId": "jreufo6zqy8uyrs12itjua"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "cellId": "14vn6etmn2hhv4bpaliwmt"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class cfg:\n",
    "    data_dir = './'\n",
    "    num_epochs = 5\n",
    "    learning_rate = 5e-5\n",
    "    batch_size = 32\n",
    "    model_name =\"cointegrated/LaBSE-en-ru\"# \"cointegrated/rubert-tiny2\"\n",
    "    dropout_prob = 0.2\n",
    "    downsample = False\n",
    "    warmup_steps = 200\n",
    "    seed=21\n",
    "    \n",
    "    \n",
    "    \n",
    "def seed_everything(seed: int):\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(cfg.seed)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "tce2l90wcbw0wqmwc82l",
    "execution_id": "3ab7f986-158e-4ea6-96fa-29633f32756a"
   },
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "cellId": "xh8uad5wiu28t9szi9a8f"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from sklearn.model_selection import GroupShuffleSplit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "cellId": "9tpjtan6y45eplvg0ehu6p"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# train_df = pd.read_csv(cfg.data_dir + 'train_train.csv')\n",
    "# test_df = pd.read_csv(cfg.data_dir + 'train_test.csv')\n",
    "\n",
    "\n",
    "train = pd.read_csv(cfg.data_dir + 'train.csv')\n",
    "\n",
    "train = train.drop_duplicates(['sentence','1category','2category'])\n",
    "\n",
    "def split_df(df):\n",
    "    splitter = GroupShuffleSplit(test_size=0.2, n_splits=2, random_state = cfg.seed)\n",
    "    split = splitter.split(df, groups=df['sentence'])\n",
    "    train_inds, test_inds = next(split)\n",
    "\n",
    "    train = df.iloc[train_inds]\n",
    "    test = df.iloc[test_inds]\n",
    "    return train,test\n",
    "\n",
    "\n",
    "train_df, test_df = split_df(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "c226apqp7vf1ivfkifjo6",
    "execution_id": "8706d6a1-ed8a-4654-9064-5422b7c9ced6"
   },
   "source": [
    "### Handle labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "cellId": "jqgy9z7mevikzp769qatp"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def handle_labels(data, multilabel=False):\n",
    "    mapping = {\n",
    "        'Communication':'0',\n",
    "        '?':'4',\n",
    "        ' ':'5',\n",
    "        'Quality':'1',\n",
    "        'Price':'2',\n",
    "        'Safety':'3'\n",
    "    }\n",
    "    data = data.copy()\n",
    "    data['2category'] = data['2category'].fillna(' ')\n",
    "    data['1category'] = data['1category'].map(mapping).astype(int)\n",
    "    data['2category'] = data['2category'].map(mapping).astype(int)\n",
    "    \n",
    "    data['cat_0'] = ((data['1category'] == 0) |(data['2category'] == 0)).astype(int)\n",
    "    data['cat_1'] = ((data['1category'] == 1) |(data['2category'] == 1)).astype(int)\n",
    "    data['cat_2'] = ((data['1category'] == 2) |(data['2category'] == 2)).astype(int)\n",
    "    data['cat_3'] = ((data['1category'] == 3) |(data['2category'] == 3)).astype(int)\n",
    "\n",
    "    data['sentiment'] = data['sentiment'].map({'+':1,'−':2,'?':0})\n",
    "\n",
    "    return data.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "cellId": "z6jt95j3y8l4pmhvfgjeif"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_df = handle_labels(train_df)\n",
    "\n",
    "test_df = handle_labels(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "cellId": "p0aks0w4270dziadl78lso"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5732 1434\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "def process_duplicates(group):\n",
    "    o = group[[\"cat_0\", \"cat_1\", \"cat_2\", \"cat_3\"]].sum(axis=0).clip(0,1)\n",
    "    return o\n",
    "gb = train_df.groupby('sentence').apply(process_duplicates)\n",
    "train_df = train_df[[\"sentence\", \"sentiment\"]].merge(gb, on='sentence').drop_duplicates(['sentence']).reset_index()\n",
    "\n",
    "gb = test_df.groupby('sentence').apply(process_duplicates)\n",
    "test_df = test_df[[\"sentence\", \"sentiment\"]].merge(gb, on='sentence').drop_duplicates(['sentence']).reset_index()\n",
    "\n",
    "print(train_df.shape[0],test_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "6gscaz64zbjw1uhtuzo3eh",
    "execution_id": "e67d3034-5a36-4989-9bc9-5ad188f1dc34",
    "id": "DR-LZpf6rIJr"
   },
   "source": [
    "### Model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "cellId": "sudv2p9jnsr0y637fkaas",
    "id": "Noe485H_rL7Z"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, text, sentiment_targets,category_targets, tokenizer, max_len):\n",
    "        self.text = text\n",
    "        self.sentiment_targets = sentiment_targets\n",
    "        self.category_targets = category_targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.text[item])\n",
    "        sentiment_target = self.sentiment_targets[item]\n",
    "        category_target = self.category_targets[item]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'sentiment_targets': torch.tensor(sentiment_target, dtype=torch.long),\n",
    "            'category_targets': torch.tensor(category_target, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "cellId": "9kfmq2bitd7dl9go4ldb64"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        config = AutoConfig.from_pretrained(cfg.model_name)\n",
    "        config.update({'output_hidden_states':True})\n",
    "        self.bert = AutoModel.from_pretrained(cfg.model_name, config=config)\n",
    "        self.drop = nn.Dropout(p=cfg.dropout_prob)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size,self.bert.config.hidden_size)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size*4, n_classes)\n",
    "        self.out_sent = nn.Linear(self.bert.config.hidden_size*4, 3)\n",
    "        self.act = nn.LeakyReLU()\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        all_hidden_states = torch.stack(outputs[2])\n",
    "\n",
    "        concatenate_pooling = torch.cat(\n",
    "            (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]),-1\n",
    "        )\n",
    "        last_hidden_state_cls = concatenate_pooling[:, 0]\n",
    "\n",
    "\n",
    "\n",
    "        #last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        x = self.drop(last_hidden_state_cls)\n",
    "        out_sent = self.out_sent(x)\n",
    "        out = self.out(x)\n",
    "\n",
    "        return out, out_sent\n",
    "    \n",
    "    \n",
    "class SentimentClassifier2(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier2, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(cfg.model_name)\n",
    "        self.drop = nn.Dropout(p=cfg.dropout_prob)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        self.out_sent = nn.Linear(self.bert.config.hidden_size, 3)\n",
    "        self.act = nn.LeakyReLU()\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         all_hidden_states = torch.stack(outputs[2])\n",
    "\n",
    "#         concatenate_pooling = torch.cat(\n",
    "#             (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]),-1\n",
    "#         )\n",
    "#         last_hidden_state_cls = concatenate_pooling[:, 0]\n",
    "\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        x = self.drop(last_hidden_state_cls)\n",
    "        out_sent = self.out_sent(x)\n",
    "        out = self.out(x)\n",
    "\n",
    "        return out, out_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "cellId": "mctvznmlun5babcatksa3"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "cellId": "vrapawuuu9je6okmgx05w",
    "id": "_hzBWCjGKASQ",
    "outputId": "94fa9448-aa27-43be-8c29-7f6fc02d394e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7646ad7ddf643bc8b3228f70eac3ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=49.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210ab90020b14f3fa4889673cce68cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=806.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be05585a1174b4d88a2cd8578637a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=521414.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129892c51e71447aa752937e89df297a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=112.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "276\n",
      "270\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
    "\n",
    "train_tokenized = [tokenizer.encode(x, add_special_tokens=True) for x in train_df.sentence]\n",
    "test_tokenized = [tokenizer.encode(x, add_special_tokens=True) for x in test_df.sentence]\n",
    "\n",
    "train_max_len = max(map(len, train_tokenized))\n",
    "test_max_len = max(map(len, test_tokenized))\n",
    "\n",
    "print(train_max_len)\n",
    "print(test_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "cellId": "2qtb8d8ybfuuppowp9eh5b",
    "id": "PriOOpje-ykn"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def create_data_loader(df, tokenizer, batch_size, max_len):\n",
    "    if 'label' in df:\n",
    "        labels = df.label.values\n",
    "    else:\n",
    "        labels = [0] * len(df)\n",
    "    ds = SentimentDataset(\n",
    "        text= df.sentence,\n",
    "        sentiment_targets= df.sentiment,\n",
    "        category_targets= df[['cat_0','cat_1','cat_2','cat_3']].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "BATCH_SIZE = cfg.batch_size\n",
    "\n",
    "train_data_loader = create_data_loader(train_df, tokenizer, BATCH_SIZE, train_max_len)\n",
    "test_data_loader = create_data_loader(test_df, tokenizer, BATCH_SIZE, test_max_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "cellId": "74hkb5tlwzk4iw7ypok66w"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# df = SentimentDataset(\n",
    "#         text= train_df.sentence,\n",
    "#         sentiment_targets= train_df.sentiment,\n",
    "#         category_targets= train_df.category_targets,\n",
    "#         tokenizer=tokenizer,\n",
    "#         max_len=230\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "y2q1mgdylmco5c4iit4o1m",
    "execution_id": "5ac58a79-de03-490d-b5ba-a77b77cd71ba",
    "id": "SNCGGRUiwp1L"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "cellId": "7qm8vjv7afrp7wpw1bvxkh",
    "id": "GgnONPoHuscw"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class CustomTrainer: \n",
    "    def __init__(self, model, train_data_loader, val_data_loader, loss_fn, optimizer, device, scheduler, n_train_examples, n_val_examples): \n",
    "        self.model = model \n",
    "        self.train_data_loader = train_data_loader \n",
    "        self.val_data_loader = val_data_loader \n",
    "        self.loss_fn = loss_fn \n",
    "        self.optimizer = optimizer \n",
    "        self.device = device \n",
    "        self.scheduler = scheduler \n",
    "        self.n_train_examples = n_train_examples \n",
    "        self.n_val_examples = n_val_examples \n",
    "\n",
    "    \n",
    "    def calc_loss(self, logits, labels):\n",
    "            loss = self.loss_fn(logits.view(-1, 4),\n",
    "                            labels.float().view(-1, 4))\n",
    "            return loss\n",
    " \n",
    "    def train_epoch(self): \n",
    "        self.model = self.model.train() \n",
    "        losses = [] \n",
    "        correct_predictions = 0 \n",
    "        for step, batch in enumerate(tqdm(self.train_data_loader, desc='TRAIN')): \n",
    "            input_ids = batch['input_ids'].to(self.device) \n",
    "            attention_mask = batch['attention_mask'].to(self.device) \n",
    "            targets = batch['category_targets'].to(self.device) \n",
    "            sentiment_targets = batch['sentiment_targets'].to(self.device) \n",
    " \n",
    "            outputs, outputs_sent = self.model(input_ids=input_ids, attention_mask=attention_mask) \n",
    "            #preds = torch.argmax(outputs, dim=1).detach()\n",
    "            preds = (outputs > 0.5).float().detach()\n",
    "            loss_cat = self.calc_loss(outputs, targets) \n",
    "            sent_loss = F.cross_entropy(outputs_sent,sentiment_targets)\n",
    "\n",
    "            loss = (2*loss_cat + sent_loss) / 3\n",
    "\n",
    "\n",
    "            correct_predictions += torch.sum(preds == targets) / 4\n",
    "            losses.append(loss.item()) \n",
    "            loss.backward() \n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=100.0) \n",
    "            self.optimizer.step() \n",
    "            self.scheduler.step() \n",
    "            self.optimizer.zero_grad() \n",
    "\n",
    "            \n",
    "\n",
    "            acc = (preds == targets).float().mean()\n",
    "\n",
    "            wandb.log({\n",
    "                'train_loss':loss.detach().cpu(),\n",
    "                'train_loss_cat':loss_cat.detach().cpu(),\n",
    "                'train_loss_sent':sent_loss.detach().cpu(),\n",
    "                'train_acc':acc.detach().cpu(),\n",
    "                'lr':self.scheduler.get_last_lr()\n",
    "\n",
    "            })\n",
    "\n",
    "\n",
    "\n",
    "        return correct_predictions.double() / self.n_train_examples, np.mean(losses)\n",
    " \n",
    "    def evaluate(self): \n",
    "        self.model = self.model.eval() \n",
    "        losses = [] \n",
    "        correct_predictions = 0 \n",
    "        trg, p = [], []\n",
    "        trg_sent, p_sent = [], []\n",
    "        for batch in tqdm(self.val_data_loader, desc='EVALUATION'): \n",
    "            input_ids = batch['input_ids'].to(self.device) \n",
    "            attention_mask = batch['attention_mask'].to(self.device) \n",
    "            targets = batch['category_targets'].to(self.device) \n",
    "            sentiment_targets = batch['sentiment_targets'].to(self.device) \n",
    "            outputs, outputs_sent = self.model(input_ids=input_ids, attention_mask=attention_mask) \n",
    "            preds = (outputs > 0.5).float().detach()\n",
    "            loss = self.calc_loss(outputs, targets) \n",
    "            correct_predictions += torch.sum(preds == targets) / 4\n",
    "            losses.append(loss.item()) \n",
    "            trg.append(targets.detach().cpu())\n",
    "            p.append(outputs.detach().cpu())\n",
    "            p_sent.append(outputs_sent.detach().cpu())\n",
    "            trg_sent.append(sentiment_targets.detach().cpu())\n",
    "\n",
    "        p = torch.cat(p, dim=0)\n",
    "        trg = torch.cat(trg, dim=0)\n",
    "        p_sent = torch.cat(p_sent, dim=0)\n",
    "        trg_sent = torch.cat(trg_sent, dim=0)\n",
    "\n",
    "        p = F.sigmoid(p)\n",
    "        roc_auc = roc_auc_score(trg, p,  multi_class = 'ovr')\n",
    "\n",
    "        p_sent = F.softmax(p_sent, dim=1)\n",
    "        roc_auc_sent = roc_auc_score(trg_sent, p_sent,  multi_class = 'ovr')\n",
    "\n",
    "        return correct_predictions.double() / self.n_val_examples, np.mean(losses), roc_auc, roc_auc_sent\n",
    "\n",
    " \n",
    "    def train(self, n_epochs): \n",
    "        wandb_conf = dict(vars(cfg))\n",
    "        del wandb_conf['__dict__']\n",
    "        del wandb_conf['__weakref__']\n",
    "        del wandb_conf['__doc__']\n",
    "        wandb.init(project = 'hack_category', config=wandb_conf)\n",
    "        for epoch in range(n_epochs): \n",
    "            print(f'Epoch {epoch + 1}/{n_epochs}') \n",
    "            print('-' * 10) \n",
    "            train_acc, train_loss = self.train_epoch() \n",
    "            print(f'Train loss {train_loss} accuracy {train_acc}') \n",
    "            val_acc, val_loss, roc_auc,roc_auc_sent = self.evaluate() \n",
    "            wandb.log({\n",
    "                'val_acc':val_acc, 'val_loss':val_loss, 'val_roc_auc':roc_auc, 'val_roc_auc_sentiment':roc_auc_sent}\n",
    "            )\n",
    "            print(f'Validation loss {val_loss} accuracy {val_acc} roc_auc {roc_auc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "cellId": "8os4bihix3eixz0ltejie",
    "id": "TOYVwIHcy1zx"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499d88da07b94cca9ac91995c631f9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=516063655.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/LaBSE-en-ru were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SentimentClassifier(4)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "EPOCHS = cfg.num_epochs\n",
    "optimizer = AdamW(model.parameters(), lr=cfg.learning_rate, correct_bias=True)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=cfg.warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model = model,\n",
    "    train_data_loader=train_data_loader, \n",
    "    val_data_loader=test_data_loader, \n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer, \n",
    "    device=device, \n",
    "    scheduler=scheduler, \n",
    "    n_train_examples=len(train_data_loader) * cfg.batch_size, \n",
    "    n_val_examples=len(test_data_loader) * cfg.batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "cellId": "mnzhj60y68r1zbpc5rws"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmikezz1\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/work/resources/wandb/run-20230409_110131-6li4v540</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mikezz1/hack_category/runs/6li4v540' target=\"_blank\">woven-bee-59</a></strong> to <a href='https://wandb.ai/mikezz1/hack_category' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mikezz1/hack_category' target=\"_blank\">https://wandb.ai/mikezz1/hack_category</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mikezz1/hack_category/runs/6li4v540' target=\"_blank\">https://wandb.ai/mikezz1/hack_category/runs/6li4v540</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333aa02ec45c4bfe928c067f79a53970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='TRAIN'), FloatProgress(value=0.0, max=180.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27aed9e91d2436c92ff740a1f003223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='EVALUATION'), FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173fd43b0ac245d0a36fc65dbd62a307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='TRAIN'), FloatProgress(value=0.0, max=180.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46fdb9622fb41acb209ad440bc03d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='EVALUATION'), FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8525ad83f4b2474c896c413769f05c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='TRAIN'), FloatProgress(value=0.0, max=180.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b31df29486498baa3dfb6560cbbf83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='EVALUATION'), FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d66067a2a74038a78d7038466848bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='TRAIN'), FloatProgress(value=0.0, max=180.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c4c2a26446494d822676b407d97325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='EVALUATION'), FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d70f5aad19984842b007b847e567fa2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='TRAIN'), FloatProgress(value=0.0, max=180.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a2de66ede74603a42e35309cfa9455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='EVALUATION'), FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 1/5\n",
      "----------\n",
      "\n",
      "Train loss 0.5056963149044249 accuracy 0.7678385416666667\n",
      "\n",
      "Validation loss 0.3407483743296729 accuracy 0.8189236111111111 roc_auc 0.7944189527272\n",
      "Epoch 2/5\n",
      "----------\n",
      "\n",
      "Train loss 0.3210673049920135 accuracy 0.8534722222222223\n",
      "\n",
      "Validation loss 0.3295520838763979 accuracy 0.8241319444444445 roc_auc 0.8523616675259907\n",
      "Epoch 3/5\n",
      "----------\n",
      "\n",
      "Train loss 0.23592150029208925 accuracy 0.8836805555555556\n",
      "\n",
      "Validation loss 0.3370925254291958 accuracy 0.8336805555555555 roc_auc 0.8634388345765001\n",
      "Epoch 4/5\n",
      "----------\n",
      "\n",
      "Train loss 0.17056634612381458 accuracy 0.9071180555555556\n",
      "\n",
      "Validation loss 0.32816669874721105 accuracy 0.851388888888889 roc_auc 0.8666692739696636\n",
      "Epoch 5/5\n",
      "----------\n",
      "\n",
      "Train loss 0.12915107496082784 accuracy 0.9274739583333333\n",
      "\n",
      "Validation loss 0.32879617081748114 accuracy 0.8598958333333334 roc_auc 0.8720570404539403\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "trainer.train(n_epochs=cfg.num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "cellId": "uez4efgigh268sy6spqdy"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "folder = \"model_laBSE-final\"\n",
    "part = 1\n",
    "torch.save(model.state_dict(),f'model_laBSE-final/model_{part}.pth')\n",
    "#torch.save(optimizer.state_dict(),'opt_laBSE-2loss-cleaned-4lidden-57.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "cellId": "tud04pa75fbee3wg858a9"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "def predict(loader, model, model_num=0):\n",
    "    model.load_state_dict(torch.load(f'model_laBSE-final/model_{model_num}.pth'))\n",
    "    model.to(device)\n",
    "    p, p_sent = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='EVALUATION'): \n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device) \n",
    "            attention_mask = batch['attention_mask'].to(device) \n",
    "            outputs, outputs_sent = model(input_ids=input_ids, attention_mask=attention_mask) \n",
    "            p.append(outputs.detach().cpu())\n",
    "            p_sent.append(outputs_sent.detach().cpu())\n",
    "\n",
    "        p = torch.cat(p, dim=0)\n",
    "        p_sent = torch.cat(p_sent, dim=0)\n",
    "\n",
    "        p = F.sigmoid(p)\n",
    "        p_sent = F.softmax(p_sent, dim=1)\n",
    "        return p, p_sent\n",
    "    \n",
    "\n",
    "# for i in range(3):\n",
    "#     model = model.load_state_dict(torch.load(f'model_laBSE-final/model_{i}.pth'))\n",
    "#     model.eval()\n",
    "#     p, p_sent = [], []\n",
    "#     for batch in tqdm(self.val_data_loader, desc='EVALUATION'): \n",
    "#         input_ids = batch['input_ids'].to(self.device) \n",
    "#         attention_mask = batch['attention_mask'].to(self.device) \n",
    "#         outputs, outputs_sent = self.model(input_ids=input_ids, attention_mask=attention_mask) \n",
    "#         p.append(outputs.detach().cpu())\n",
    "#         p_sent.append(outputs_sent.detach().cpu())\n",
    "\n",
    "#     # p - (n, 4), p_sent - (n, 4)\n",
    "#     p = torch.cat(p, dim=0)\n",
    "#     p_sent = torch.cat(p_sent, dim=0)\n",
    "\n",
    "#     p = F.sigmoid(p)\n",
    "#     p_sent = F.softmax(p_sent, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "cellId": "4sr7jge2x9yxxmo9tk17e"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "cellId": "405h7s62gjk5kwfscsm4lc"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "new_df = pd.read_csv('data_participants_with_probs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "cellId": "4khgmtg5skb05ur7vl28bjw"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>sentence</th>\n",
       "      <th>prob_?</th>\n",
       "      <th>prob_+</th>\n",
       "      <th>prob_-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.03.2022 обратился на горячую линию для закр...</td>\n",
       "      <td>0.999298</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Уже который год в ТКБ не решается \"глобальная ...</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.999918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Добрый день</td>\n",
       "      <td>0.998809</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.000203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Добрый день Сегодня, зайдя в свой личный кабин...</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.999855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Обслуживаюсь в Тинькофф пару лет, возникла жес...</td>\n",
       "      <td>0.999036</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.000230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>944</td>\n",
       "      <td>944</td>\n",
       "      <td>944</td>\n",
       "      <td>Отвратительный сервис и отношение к клиентам! ...</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.999927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>945</td>\n",
       "      <td>945</td>\n",
       "      <td>945</td>\n",
       "      <td>28.04.2022 обратилась в банк о возможности пер...</td>\n",
       "      <td>0.999307</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.000316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>946</td>\n",
       "      <td>946</td>\n",
       "      <td>946</td>\n",
       "      <td>В начале 2021 года была акция по выплате 8% ке...</td>\n",
       "      <td>0.999454</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>947</td>\n",
       "      <td>947</td>\n",
       "      <td>947</td>\n",
       "      <td>Бездействие банка и некомпетентность сотрудников</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.999930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>948</td>\n",
       "      <td>948</td>\n",
       "      <td>948</td>\n",
       "      <td>Потрачено 5 часов чтобы произвести оплату за о...</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.999868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>949 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  ...    prob_?    prob_+    prob_-\n",
       "0             0             0               0  ...  0.999298  0.000370  0.000332\n",
       "1             1             1               1  ...  0.000055  0.000026  0.999918\n",
       "2             2             2               2  ...  0.998809  0.000988  0.000203\n",
       "3             3             3               3  ...  0.000113  0.000032  0.999855\n",
       "4             4             4               4  ...  0.999036  0.000734  0.000230\n",
       "..          ...           ...             ...  ...       ...       ...       ...\n",
       "944         944           944             944  ...  0.000048  0.000025  0.999927\n",
       "945         945           945             945  ...  0.999307  0.000377  0.000316\n",
       "946         946           946             946  ...  0.999454  0.000320  0.000225\n",
       "947         947           947             947  ...  0.000043  0.000027  0.999930\n",
       "948         948           948             948  ...  0.000095  0.000037  0.999868\n",
       "\n",
       "[949 rows x 7 columns]"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "cellId": "1tif2ihzgq6926u90cl81"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "new_df = pd.read_csv('data_participants_with_probs.csv')\n",
    "new_df['category_targets'] = 0\n",
    "new_df['sentiment'] = 0\n",
    "new_df['cat_0']=0\n",
    "new_df['cat_1']=0\n",
    "new_df['cat_2']=0\n",
    "new_df['cat_3']=0\n",
    "new_data = create_data_loader(new_df, tokenizer, BATCH_SIZE, train_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "cellId": "mjyz2g85holc0612mg3yi"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "cellId": "8y4y4x4bn34r07g3yekkvr"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "cellId": "2hxz03xjfnxzghyx44kdy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/LaBSE-en-ru were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4370e800222449cca396b185dd1ae3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='EVALUATION'), FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "p0,sent_p0 = predict(new_data, model=SentimentClassifier(4), model_num=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "cellId": "xnwbr1eoajdhhaqntbuq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/LaBSE-en-ru were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59908968587841a6a08ac0aff3eb0205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='EVALUATION'), FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "p1, sent_p1 = predict(new_data, model=SentimentClassifier(4), model_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "cellId": "5jglea2u4ut1fqnsv9x2c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/LaBSE-en-ru were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa703d3d7aba40e08aed03987d5b751e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='EVALUATION'), FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "p2, sent_p2 = predict(new_data, model=SentimentClassifier(4), model_num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "cellId": "4lnxwj1i7q5ap15kxjmd9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.8796e-01, 1.8422e-03, 1.0195e-02],\n",
       "        [2.8869e-03, 4.2184e-04, 9.9669e-01],\n",
       "        [9.3149e-01, 3.5332e-02, 3.3180e-02],\n",
       "        ...,\n",
       "        [9.8275e-01, 1.5016e-02, 2.2339e-03],\n",
       "        [1.1529e-03, 3.9518e-03, 9.9490e-01],\n",
       "        [3.9336e-03, 1.5961e-03, 9.9447e-01]])"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "preds_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "cellId": "2rxh49xrfsdmvgeom0dnzf"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "preds_sent = torch.stack([sent_p0,sent_p1,sent_p2]).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "cellId": "ksuurz99qhqo0miofy53u8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31524773203070483"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "(train_df[['cat_0', 'cat_1', 'cat_2','cat_3']].sum(axis=1) > 1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "cellId": "xic5ybmla0hnenj1v50i4g"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "preds_cat = torch.stack([p0,p1,p2]).mean(dim=0)\n",
    "new_df['communication'] = preds_cat[:, 0]\n",
    "new_df['quality'] = preds_cat[:, 1]\n",
    "new_df['price'] = preds_cat[:, 2]\n",
    "new_df['safety'] = preds_cat[:, 3]\n",
    "\n",
    "new_df['my_prob_?'] = preds_sent[:, 0]\n",
    "new_df['my_prob_+'] = preds_sent[:, 1]\n",
    "new_df['my_prob_-'] = preds_sent[:, 2]\n",
    "\n",
    "new_df['?'] = (new_df['my_prob_?'] + new_df['prob_?']) / 2\n",
    "new_df['+'] = (new_df['my_prob_+'] + new_df['prob_+']) / 2\n",
    "new_df['-'] = (new_df['my_prob_-'] + new_df['prob_-']) / 2\n",
    "\n",
    "new_df['second_category'] = ((new_df[['cat_Communication_proba', 'cat_Quality_proba', 'cat_Price_proba','cat_Safety_proba']] > 0.5).sum(axis=1) > 1).astype(int)\n",
    "\n",
    "\n",
    "new_df[['sentence', 'communication',\n",
    "        'quality','price',\n",
    "        'safety','?','-','+','second_category']].to_csv('test_scored_cats_mike_vFF.csv')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "ecba08db-73b1-4d9a-991f-cfb3f8ccff83",
  "notebookPath": "bert.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
